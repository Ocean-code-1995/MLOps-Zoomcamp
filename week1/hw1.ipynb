{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Homework`***\n",
    "---\n",
    "\n",
    "The goal of this homework is to train a simple model for predicting the duration of a ride - similar to what we did in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "feb23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q1. Downloading the data***\n",
    "---\n",
    "\n",
    "We'll use the same NYC taxi dataset, but instead of \"Green Taxi Trip Records\", we'll use \"Yellow Taxi Trip Records\".\n",
    "\n",
    "Download the data for January and February 2023.\n",
    "\n",
    "Read the data for January. How many columns are there?\n",
    "\n",
    "- 16\n",
    "- 17\n",
    "- 18\n",
    "- 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns: {jan23.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "---\n",
    "\n",
    "Now let's compute the duration variable. It should contain the duration of a ride in minutes.\n",
    "\n",
    "What's the standard deviation of the trips duration in January?\n",
    "\n",
    "- 32.59\n",
    "- 42.59\n",
    "- 52.59\n",
    "- 62.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.066766e+06\n",
       "mean     1.566900e+01\n",
       "std      4.259435e+01\n",
       "min     -2.920000e+01\n",
       "25%      7.116667e+00\n",
       "50%      1.151667e+01\n",
       "75%      1.830000e+01\n",
       "max      1.002918e+04\n",
       "Name: duration_minutes, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate duration in minutes\n",
    "jan23['duration_minutes'] = (jan23['tpep_dropoff_datetime'] - jan23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "feb23['duration_minutes'] = (feb23['tpep_dropoff_datetime'] - feb23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "jan23['duration_minutes'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q3. Dropping outliers***\n",
    "---\n",
    "\n",
    "Next, we need to check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "What fraction of the records left after you dropped the outliers?\n",
    "\n",
    "- 90%\n",
    "- 92%\n",
    "- 95%\n",
    "- 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of trips that are between 1 and 60 minutes: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Filter out trips that are less than 1 minute and more than 60 minutes\n",
    "mask_jan = (jan23['duration_minutes'] >= 1) & (jan23['duration_minutes'] <= 60)\n",
    "jan23_clean = jan23[mask_jan].copy()\n",
    "\n",
    "mask_feb = (feb23['duration_minutes'] >= 1) & (feb23['duration_minutes'] <= 60)\n",
    "feb23_clean = feb23[mask_feb].copy()\n",
    "\n",
    "# Calculate and print the fraction of trips that are between 1 and 60 minutes\n",
    "fraction = jan23_clean.shape[0] / jan23.shape[0]\n",
    "print(f\"Fraction of trips that are between 1 and 60 minutes: {fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q4. One-hot encoding***\n",
    "---\n",
    "\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "\n",
    "Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "Fit a dictionary vectorizer\n",
    "Get a feature matrix from it\n",
    "What's the dimensionality of this matrix (number of columns)?\n",
    "\n",
    "- 2\n",
    "- 155\n",
    "- 345\n",
    "- 515\n",
    "- 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 515\n"
     ]
    }
   ],
   "source": [
    "# Filter columns\n",
    "columns = ['PULocationID', 'DOLocationID', 'duration_minutes']\n",
    "jan23_clean = jan23_clean[columns]\n",
    "feb23_clean = feb23_clean[columns]\n",
    "\n",
    "# Cast to str\n",
    "jan23_clean['PULocationID'] = jan23_clean['PULocationID'].astype(str)\n",
    "jan23_clean['DOLocationID'] = jan23_clean['DOLocationID'].astype(str)\n",
    "feb23_clean['PULocationID'] = feb23_clean['PULocationID'].astype(str)\n",
    "feb23_clean['DOLocationID'] = feb23_clean['DOLocationID'].astype(str)\n",
    "\n",
    "# Define train and validation sets\n",
    "X_train, y_train = jan23_clean.drop(columns='duration_minutes'), jan23_clean['duration_minutes']\n",
    "X_test, y_test = feb23_clean.drop(columns='duration_minutes'), feb23_clean['duration_minutes']\n",
    "\n",
    "# Vectorize\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = X_train.to_dict(orient='records')\n",
    "X_train_encoded = dv.fit_transform(train_dict)\n",
    "\n",
    "test_dict = X_test.to_dict(orient='records')\n",
    "X_test_encoded = dv.transform(test_dict)\n",
    "\n",
    "# Print number of columns\n",
    "print(f\"Number of columns: {X_train_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q5. Training a model***\n",
    "---\n",
    "\n",
    "Now let's use the feature matrix from the previous step to train a model.\n",
    "\n",
    "Train a plain linear regression model with default parameters\n",
    "Calculate the RMSE of the model on the training data\n",
    "What's the RMSE on train?\n",
    "\n",
    "- 3.64\n",
    "- 7.64\n",
    "- 11.64\n",
    "- 16.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m      6\u001b[0m y_pred_train \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train_encoded)\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/AI_env/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/AI_env/lib/python3.10/site-packages/sklearn/linear_model/_base.py:651\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outs])\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingular_ \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/AI_env/lib/python3.10/site-packages/scipy/linalg/_basic.py:1212\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlstsq\u001b[39m(a, b, cond\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1102\u001b[0m           check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, lapack_driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m    Compute least-squares solution to equation Ax = b.\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1210\u001b[0m \n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1212\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_validated\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m     b1 \u001b[38;5;241m=\u001b[39m _asarray_validated(b, check_finite\u001b[38;5;241m=\u001b[39mcheck_finite)\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/AI_env/lib/python3.10/site-packages/scipy/_lib/_util.py:321\u001b[0m, in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked arrays are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    320\u001b[0m toarray \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite \u001b[38;5;28;01mif\u001b[39;00m check_finite \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray\n\u001b[0;32m--> 321\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m objects_ok:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Developer/miniconda/base/envs/AI_env/lib/python3.10/site-packages/numpy/lib/function_base.py:629\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert the input to an array, checking for NaNs or Infs.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = model.predict(X_train_encoded)\n",
    "\n",
    "# Evaluate\n",
    "rmse = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "train_rmse = rmse(y_train, y_pred_train)\n",
    "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "test_rmse = rmse(y_test, y_pred_test)\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q6. Evaluating the model***\n",
    "---\n",
    "\n",
    "Now let's apply this model to the validation dataset (February 2023).\n",
    "\n",
    "What's the RMSE on validation?\n",
    "\n",
    "- 3.81\n",
    "- 7.81\n",
    "- 11.81\n",
    "- 16.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test_encoded)\n",
    "rmse(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to eventually solve kernel crash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of trips that are between 1 and 60 minutes: 0.98\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "jan23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "feb23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')\n",
    "\n",
    "# Calculate duration in minutes\n",
    "jan23['duration_minutes'] = (jan23['tpep_dropoff_datetime'] - jan23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "feb23['duration_minutes'] = (feb23['tpep_dropoff_datetime'] - feb23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Filter out trips that are less than 1 minute and more than 60 minutes\n",
    "mask_jan = (jan23['duration_minutes'] >= 1) & (jan23['duration_minutes'] <= 60)\n",
    "jan23_clean = jan23[mask_jan].copy()\n",
    "\n",
    "mask_feb = (feb23['duration_minutes'] >= 1) & (feb23['duration_minutes'] <= 60)\n",
    "feb23_clean = feb23[mask_feb].copy()\n",
    "\n",
    "# Calculate and print the fraction of trips that are between 1 and 60 minutes\n",
    "fraction = jan23_clean.shape[0] / jan23.shape[0]\n",
    "print(f\"Fraction of trips that are between 1 and 60 minutes: {fraction:.2f}\")\n",
    "\n",
    "# Filter columns\n",
    "columns = ['PULocationID', 'DOLocationID', 'duration_minutes']\n",
    "jan23_clean = jan23_clean[columns]\n",
    "feb23_clean = feb23_clean[columns]\n",
    "\n",
    "# Define train and validation sets\n",
    "X_train, y_train = jan23_clean.drop(columns='duration_minutes'), jan23_clean['duration_minutes']\n",
    "X_test, y_test = feb23_clean.drop(columns='duration_minutes'), feb23_clean['duration_minutes']\n",
    "\n",
    "# OneHotEncode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # Handle unknown categories\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(X_train[['PULocationID', 'DOLocationID']])\n",
    "X_test_encoded = encoder.transform(X_test[['PULocationID', 'DOLocationID']])\n",
    "\n",
    "# Convert to dense array\n",
    "X_train_encoded = X_train_encoded.toarray()\n",
    "X_test_encoded = X_test_encoded.toarray()\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict and evaluate on the training set\n",
    "y_pred_train = model.predict(X_train_encoded)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_convert_data(file_path):\n",
    "    # Load data\n",
    "    data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Convert float64 columns to float32\n",
    "    for col in data.select_dtypes(include=['float64']).columns:\n",
    "        data[col] = data[col].astype('float16')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the data\n",
    "logging.info(\"Loading January data\")\n",
    "jan23 = load_and_convert_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "logging.info(\"Loading February data\")\n",
    "feb23 = load_and_convert_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')\n",
    "\n",
    "# Calculate duration in minutes\n",
    "logging.info(\"Calculating duration in minutes for January data\")\n",
    "jan23['duration_minutes'] = (jan23['tpep_dropoff_datetime'] - jan23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "logging.info(\"Calculating duration in minutes for February data\")\n",
    "feb23['duration_minutes'] = (feb23['tpep_dropoff_datetime'] - feb23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Filter out trips that are less than 1 minute and more than 60 minutes\n",
    "logging.info(\"Filtering January data\")\n",
    "mask_jan = (jan23['duration_minutes'] >= 1) & (jan23['duration_minutes'] <= 60)\n",
    "jan23_clean = jan23[mask_jan].copy()\n",
    "\n",
    "logging.info(\"Filtering February data\")\n",
    "mask_feb = (feb23['duration_minutes'] >= 1) & (feb23['duration_minutes'] <= 60)\n",
    "feb23_clean = feb23[mask_feb].copy()\n",
    "\n",
    "# Calculate and print the fraction of trips that are between 1 and 60 minutes\n",
    "fraction = jan23_clean.shape[0] / jan23.shape[0]\n",
    "logging.info(f\"Fraction of trips that are between 1 and 60 minutes: {fraction:.2f}\")\n",
    "\n",
    "# Filter columns\n",
    "logging.info(\"Filtering columns\")\n",
    "columns = ['PULocationID', 'DOLocationID', 'duration_minutes']\n",
    "jan23_clean = jan23_clean[columns]\n",
    "feb23_clean = feb23_clean[columns]\n",
    "\n",
    "# Define train and validation sets\n",
    "logging.info(\"Defining train and test sets\")\n",
    "X_train, y_train = jan23_clean.drop(columns='duration_minutes'), jan23_clean['duration_minutes']\n",
    "X_test, y_test = feb23_clean.drop(columns='duration_minutes'), feb23_clean['duration_minutes']\n",
    "\n",
    "# OneHotEncode categorical features\n",
    "logging.info(\"One-hot encoding categorical features\")\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(X_train[['PULocationID', 'DOLocationID']])\n",
    "X_test_encoded = encoder.transform(X_test[['PULocationID', 'DOLocationID']])\n",
    "\n",
    "# Convert to dense array\n",
    "logging.info(\"Converting to dense arrays\")\n",
    "X_train_encoded = X_train_encoded.toarray()\n",
    "X_test_encoded = X_test_encoded.toarray()\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Training the model\")\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict and evaluate on the training set\n",
    "logging.info(\"Making predictions on the training set\")\n",
    "y_pred_train = model.predict(X_train_encoded)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "logging.info(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "logging.info(\"Making predictions on the test set\")\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "logging.info(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
