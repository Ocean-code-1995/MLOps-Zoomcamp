{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***`Homework`***\n",
    "---\n",
    "\n",
    "The goal of this homework is to train a simple model for predicting the duration of a ride - similar to what we did in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "feb23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q1. Downloading the data***\n",
    "---\n",
    "\n",
    "We'll use the same NYC taxi dataset, but instead of \"Green Taxi Trip Records\", we'll use \"Yellow Taxi Trip Records\".\n",
    "\n",
    "Download the data for January and February 2023.\n",
    "\n",
    "Read the data for January. How many columns are there?\n",
    "\n",
    "- 16\n",
    "- 17\n",
    "- 18\n",
    "- 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 19\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of columns: {jan23.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Computing duration\n",
    "---\n",
    "\n",
    "Now let's compute the duration variable. It should contain the duration of a ride in minutes.\n",
    "\n",
    "What's the standard deviation of the trips duration in January?\n",
    "\n",
    "- 32.59\n",
    "- 42.59\n",
    "- 52.59\n",
    "- 62.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.066766e+06\n",
       "mean     1.566900e+01\n",
       "std      4.259435e+01\n",
       "min     -2.920000e+01\n",
       "25%      7.116667e+00\n",
       "50%      1.151667e+01\n",
       "75%      1.830000e+01\n",
       "max      1.002918e+04\n",
       "Name: duration_minutes, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate duration in minutes\n",
    "jan23['duration_minutes'] = (jan23['tpep_dropoff_datetime'] - jan23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "feb23['duration_minutes'] = (feb23['tpep_dropoff_datetime'] - feb23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "jan23['duration_minutes'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q3. Dropping outliers***\n",
    "---\n",
    "\n",
    "Next, we need to check the distribution of the duration variable. There are some outliers. Let's remove them and keep only the records where the duration was between 1 and 60 minutes (inclusive).\n",
    "\n",
    "What fraction of the records left after you dropped the outliers?\n",
    "\n",
    "- 90%\n",
    "- 92%\n",
    "- 95%\n",
    "- 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of trips that are between 1 and 60 minutes: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Filter out trips that are less than 1 minute and more than 60 minutes\n",
    "mask_jan = (jan23['duration_minutes'] >= 1) & (jan23['duration_minutes'] <= 60)\n",
    "jan23_clean = jan23[mask_jan].copy()\n",
    "\n",
    "mask_feb = (feb23['duration_minutes'] >= 1) & (feb23['duration_minutes'] <= 60)\n",
    "feb23_clean = feb23[mask_feb].copy()\n",
    "\n",
    "# Calculate and print the fraction of trips that are between 1 and 60 minutes\n",
    "fraction = jan23_clean.shape[0] / jan23.shape[0]\n",
    "print(f\"Fraction of trips that are between 1 and 60 minutes: {fraction:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q4. One-hot encoding***\n",
    "---\n",
    "\n",
    "Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model.\n",
    "\n",
    "Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)\n",
    "Fit a dictionary vectorizer\n",
    "Get a feature matrix from it\n",
    "What's the dimensionality of this matrix (number of columns)?\n",
    "\n",
    "- 2\n",
    "- 155\n",
    "- 345\n",
    "- 515\n",
    "- 715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 515\n"
     ]
    }
   ],
   "source": [
    "# Filter columns\n",
    "columns = ['PULocationID', 'DOLocationID', 'duration_minutes']\n",
    "jan23_clean = jan23_clean[columns]\n",
    "feb23_clean = feb23_clean[columns]\n",
    "\n",
    "# Cast to str\n",
    "jan23_clean['PULocationID'] = jan23_clean['PULocationID'].astype(str)\n",
    "jan23_clean['DOLocationID'] = jan23_clean['DOLocationID'].astype(str)\n",
    "feb23_clean['PULocationID'] = feb23_clean['PULocationID'].astype(str)\n",
    "feb23_clean['DOLocationID'] = feb23_clean['DOLocationID'].astype(str)\n",
    "\n",
    "# Define train and validation sets\n",
    "X_train, y_train = jan23_clean.drop(columns='duration_minutes'), jan23_clean['duration_minutes']\n",
    "X_test, y_test = feb23_clean.drop(columns='duration_minutes'), feb23_clean['duration_minutes']\n",
    "\n",
    "# Vectorize\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = X_train.to_dict(orient='records')\n",
    "X_train_encoded = dv.fit_transform(train_dict)\n",
    "\n",
    "test_dict = X_test.to_dict(orient='records')\n",
    "X_test_encoded = dv.transform(test_dict)\n",
    "\n",
    "# Print number of columns\n",
    "print(f\"Number of columns: {X_train_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q5. Training a model***\n",
    "---\n",
    "\n",
    "Now let's use the feature matrix from the previous step to train a model.\n",
    "\n",
    "Train a plain linear regression model with default parameters\n",
    "Calculate the RMSE of the model on the training data\n",
    "What's the RMSE on train?\n",
    "\n",
    "- 3.64\n",
    "- 7.64\n",
    "- 11.64\n",
    "- 16.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = model.predict(X_train_encoded)\n",
    "\n",
    "# Evaluate\n",
    "rmse = lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "train_rmse = rmse(y_train, y_pred_train)\n",
    "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "test_rmse = rmse(y_test, y_pred_test)\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Q6. Evaluating the model***\n",
    "---\n",
    "\n",
    "Now let's apply this model to the validation dataset (February 2023).\n",
    "\n",
    "What's the RMSE on validation?\n",
    "\n",
    "- 3.81\n",
    "- 7.81\n",
    "- 11.81\n",
    "- 16.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test_encoded)\n",
    "rmse(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to eventually solve kernel crash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of trips that are between 1 and 60 minutes: 0.98\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data\n",
    "jan23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet')\n",
    "feb23 = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet')\n",
    "\n",
    "# Calculate duration in minutes\n",
    "jan23['duration_minutes'] = (jan23['tpep_dropoff_datetime'] - jan23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "feb23['duration_minutes'] = (feb23['tpep_dropoff_datetime'] - feb23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Filter out trips that are less than 1 minute and more than 60 minutes\n",
    "mask_jan = (jan23['duration_minutes'] >= 1) & (jan23['duration_minutes'] <= 60)\n",
    "jan23_clean = jan23[mask_jan].copy()\n",
    "\n",
    "mask_feb = (feb23['duration_minutes'] >= 1) & (feb23['duration_minutes'] <= 60)\n",
    "feb23_clean = feb23[mask_feb].copy()\n",
    "\n",
    "# Calculate and print the fraction of trips that are between 1 and 60 minutes\n",
    "fraction = jan23_clean.shape[0] / jan23.shape[0]\n",
    "print(f\"Fraction of trips that are between 1 and 60 minutes: {fraction:.2f}\")\n",
    "\n",
    "# Filter columns\n",
    "columns = ['PULocationID', 'DOLocationID', 'duration_minutes']\n",
    "jan23_clean = jan23_clean[columns]\n",
    "feb23_clean = feb23_clean[columns]\n",
    "\n",
    "# Define train and validation sets\n",
    "X_train, y_train = jan23_clean.drop(columns='duration_minutes'), jan23_clean['duration_minutes']\n",
    "X_test, y_test = feb23_clean.drop(columns='duration_minutes'), feb23_clean['duration_minutes']\n",
    "\n",
    "# OneHotEncode categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # Handle unknown categories\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(X_train[['PULocationID', 'DOLocationID']])\n",
    "X_test_encoded = encoder.transform(X_test[['PULocationID', 'DOLocationID']])\n",
    "\n",
    "# Convert to dense array\n",
    "X_train_encoded = X_train_encoded.toarray()\n",
    "X_test_encoded = X_test_encoded.toarray()\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict and evaluate on the training set\n",
    "y_pred_train = model.predict(X_train_encoded)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_convert_data(file_path, columns):\n",
    "    # Load data\n",
    "    data = pd.read_parquet(file_path, columns=columns, engine='pyarrow')\n",
    "    \n",
    "    # Convert float64 columns to float32\n",
    "    for col in data.select_dtypes(include=['float64']).columns:\n",
    "        data[col] = data[col].astype('float16')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 09:59:10,757 - INFO - Loading January data\n",
      "2024-05-15 09:59:11,269 - INFO - Loading February data\n",
      "2024-05-15 09:59:11,557 - INFO - Calculating duration in minutes for January data\n",
      "2024-05-15 09:59:11,659 - INFO - Calculating duration in minutes for February data\n",
      "2024-05-15 09:59:11,721 - INFO - Filtering January data\n",
      "2024-05-15 09:59:11,947 - INFO - Filtering February data\n",
      "2024-05-15 09:59:12,134 - INFO - Fraction of trips that are between 1 and 60 minutes: 0.98\n",
      "2024-05-15 09:59:12,135 - INFO - Filtering columns\n",
      "2024-05-15 09:59:12,180 - INFO - Defining train and test sets\n",
      "2024-05-15 09:59:12,207 - INFO - One-hot encoding categorical features\n",
      "/home/basti/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "2024-05-15 09:59:13,774 - INFO - Training the model\n",
      "2024-05-15 10:00:07,336 - INFO - Making predictions on the training set\n",
      "2024-05-15 10:00:07,372 - INFO - Train RMSE: 7.65\n",
      "2024-05-15 10:00:07,373 - INFO - Making predictions on the test set\n",
      "2024-05-15 10:00:07,406 - INFO - Test RMSE: 7.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the data\n",
    "logging.info(\"Loading January data\")\n",
    "\n",
    "cols_to_read = ['tpep_dropoff_datetime', 'tpep_pickup_datetime', 'PULocationID', 'DOLocationID']\n",
    "\n",
    "jan23 = load_and_convert_data(\n",
    "    file_path='https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet',\n",
    "    columns=cols_to_read\n",
    ")\n",
    "logging.info(\"Loading February data\")\n",
    "feb23 = load_and_convert_data(\n",
    "    file_path='https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet',\n",
    "    columns=cols_to_read\n",
    ")\n",
    "\n",
    "# Calculate duration in minutes\n",
    "logging.info(\"Calculating duration in minutes for January data\")\n",
    "jan23['duration_minutes'] = (jan23['tpep_dropoff_datetime'] - jan23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "logging.info(\"Calculating duration in minutes for February data\")\n",
    "feb23['duration_minutes'] = (feb23['tpep_dropoff_datetime'] - feb23['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Filter out trips that are less than 1 minute and more than 60 minutes\n",
    "logging.info(\"Filtering January data\")\n",
    "mask_jan = (jan23['duration_minutes'] >= 1) & (jan23['duration_minutes'] <= 60)\n",
    "jan23_clean = jan23[mask_jan].copy()\n",
    "\n",
    "logging.info(\"Filtering February data\")\n",
    "mask_feb = (feb23['duration_minutes'] >= 1) & (feb23['duration_minutes'] <= 60)\n",
    "feb23_clean = feb23[mask_feb].copy()\n",
    "\n",
    "# Calculate and print the fraction of trips that are between 1 and 60 minutes\n",
    "fraction = jan23_clean.shape[0] / jan23.shape[0]\n",
    "logging.info(f\"Fraction of trips that are between 1 and 60 minutes: {fraction:.2f}\")\n",
    "\n",
    "# Filter columns\n",
    "logging.info(\"Filtering columns\")\n",
    "columns = ['PULocationID', 'DOLocationID', 'duration_minutes']\n",
    "jan23_clean = jan23_clean[columns]\n",
    "feb23_clean = feb23_clean[columns]\n",
    "\n",
    "# Define train and validation sets\n",
    "logging.info(\"Defining train and test sets\")\n",
    "X_train, y_train = jan23_clean.drop(columns='duration_minutes'), jan23_clean['duration_minutes']\n",
    "X_test, y_test = feb23_clean.drop(columns='duration_minutes'), feb23_clean['duration_minutes']\n",
    "\n",
    "# OneHotEncode categorical features\n",
    "logging.info(\"One-hot encoding categorical features\")\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(X_train[['PULocationID', 'DOLocationID']])\n",
    "X_test_encoded = encoder.transform(X_test[['PULocationID', 'DOLocationID']])\n",
    "\n",
    "# Convert to dense array\n",
    "#logging.info(\"Converting to dense arrays\")\n",
    "#X_train_encoded = X_train_encoded.toarray()\n",
    "#X_test_encoded = X_test_encoded.toarray()\n",
    "\n",
    "# Train the model\n",
    "logging.info(\"Training the model\")\n",
    "try: \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "except Exception as e:\n",
    "    logging.error(\"Operation failed:\", exc_info=True)\n",
    "\n",
    "# Predict and evaluate on the training set\n",
    "logging.info(\"Making predictions on the training set\")\n",
    "y_pred_train = model.predict(X_train_encoded)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "logging.info(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "logging.info(\"Making predictions on the test set\")\n",
    "y_pred_test = model.predict(X_test_encoded)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "logging.info(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
